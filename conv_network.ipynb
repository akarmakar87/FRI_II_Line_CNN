{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY WORKING IN THIS ONE\n",
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn import preprocessing as p\n",
    "import gc\n",
    "import cv2\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = p.MinMaxScaler()\n",
    "INPUT_DATA_PATH = 'test_input_new.npy'\n",
    "OUTPUT_DATA_PATH = 'test_output_new.npy'\n",
    "x_input = np.load(INPUT_DATA_PATH)\n",
    "y_input = np.load(OUTPUT_DATA_PATH)\n",
    "\n",
    "x_min = x_input.min(axis=(1, 2), keepdims=True)\n",
    "x_max = x_input.max(axis=(1, 2), keepdims=True)\n",
    "x_input = (x_input - x_min)/(x_max-x_min)\n",
    "\n",
    "y_min = y_input.min(axis=(1, 2), keepdims=True)\n",
    "y_max = y_input.max(axis=(1, 2), keepdims=True)\n",
    "y_input = (y_input - y_min)/(y_max-y_min)\n",
    "\n",
    "num_points = np.load('test_nums.npy')\n",
    "# print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_input[x_input < 1] = 0\n",
    "\n",
    "\n",
    "# print(x_input[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(tensor):\n",
    "    for img in range(len(tensor)):\n",
    "            for channel in range(1):\n",
    "                    for h in range(100):\n",
    "                            for w in range(100):\n",
    "                                    if (tensor[img][channel][h][w] < 1):\n",
    "                                            tensor[img][channel][h][w] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "SPLIT_IDX = 7500\n",
    "x_train = torch.tensor(x_input[:SPLIT_IDX], dtype=torch.float).permute(0,3,1,2)\n",
    "x_test = torch.tensor(x_input[SPLIT_IDX:], dtype=torch.float).permute(0,3,1,2)\n",
    "\n",
    "y_train = torch.tensor(y_input[:SPLIT_IDX], dtype=torch.float).permute(0,3,1,2)\n",
    "y_test = torch.tensor(y_input[SPLIT_IDX:], dtype=torch.float).permute(0,3,1,2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "test_num_points = num_points[SPLIT_IDX:]\n",
    "\n",
    "# print (test_num_points.shape)\n",
    "\n",
    "# threshold(x_train)                       \n",
    "# threshold(x_test)\n",
    "# threshold(y_train)                       \n",
    "# threshold(y_test)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(x_train,y_train) # create your datset\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=True) # create your dataloader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True) # create your dataloader\n",
    "\n",
    "print (len(train_dataloader))\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "#/home/bwilab/asha_ritu/line_model/conv_network.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CNN\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.cnn_stack = nn.Sequential( \n",
    "            # first layer: learning one person. \n",
    "            # kernel size = diameter of a dot (5 pixels) + padding (2 pixels) = (7,7)\n",
    "            # out_channels: 1 for now\n",
    "            # try changing the out_channels\n",
    "            # larger dataset and ReLU between each layer\n",
    "            # larger dataset, more layers, larger kernels\n",
    "\n",
    "            # find center pixel of each circle, count number of dots\n",
    "\n",
    "            # blob detect: looking for continuous regions of non-white pixels\n",
    "            # confidence regions. \n",
    "            # masking the image like in hw3\n",
    "            # \n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(6,6), stride=1, bias=True, padding=(2,2)), # input layer\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(11,11), stride=1, bias=True, padding=(5,5)), # input layer\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(19,19), stride=1, bias=True, padding=(9,9)), # input layer\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(11,11), stride=1, bias=True, padding=(5,5)), # input layer\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            # layer 2: learning two dots.\n",
    "            # kernel_size = diameter of two dots (10 pixels) + distance between them (2 pixels) + padding (2 pixels)\n",
    "            # there are 8 different orientations two people can be in, resulting in 8 output channels.\n",
    "            # two options: either increase kernel size and number of channels or\n",
    "\n",
    "            # pool -> lower image resolution and learn a smaller pattern\n",
    "\n",
    "            # nn.MaxPool2d()\n",
    "\n",
    "\n",
    "            # size of output channel: [(input size + kernel size + padding top + padding bottom) / stride] + 1\n",
    "\n",
    "            # nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(12, 12), stride=1, bias=True, padding=(8,8)),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "\n",
    "\n",
    "            # nn.Conv2d(in_channels=8, out_channels=10, kernel_size=(15, 15), stride=1, bias=True, padding=(8,8)),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "\n",
    "            # final layer: out_channels=1, \n",
    "            # kernel size = ? ask Dr. Hart what a reasonable kernel size for the output layer could be.\n",
    "            nn.Conv2d(in_channels=10, out_channels=1, kernel_size=(6,6), padding=(3,3)), # 100 x 100\n",
    "            nn.ReLU()\n",
    "            # nn.Sigmoid() # values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.cnn_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "learning_rate = 3e-4\n",
    "weight_decay=1e-5\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), weight_decay=weight_decay, lr=learning_rate)\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.CrossEntropyLoss() # can change this to another loss function\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    avg_loss = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "                  \n",
    "        # print(X)\n",
    "        pred = model(X)\n",
    "        # print(f\"pred:{pred}\")\n",
    "        # print(f\"target:{y}\")\n",
    "    \n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"y shape:{y.shape}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "        avg_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch % 100 == 0:\n",
    "        #     print(f\"loss: {loss:>7f}\")\n",
    "    avg_loss = avg_loss / size\n",
    "    print(f\"average loss: {avg_loss}\")\n",
    "\n",
    "        # if batch % 50 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n"
     ]
    }
   ],
   "source": [
    "size = len(train_dataloader.dataset)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(model_input, model_output):\n",
    "    WHITE = 1\n",
    "    BLACK = 0\n",
    "\n",
    "    model_output = 1-model_output\n",
    "    model_input = 1-model_input\n",
    "    model_input[model_input != WHITE] = BLACK\n",
    "\n",
    "    masked_arr = np.logical_and(model_input, model_output, model_input == WHITE)\n",
    "    return masked_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    avg_accuracy = 0\n",
    "    size = len(test_dataloader.dataset)\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            # print(output*255)\n",
    "            for out_i in range(100):\n",
    "                \n",
    "                masked_arr = mask(np.array(data.cpu().data[out_i][0]), np.array(output.cpu().data[out_i][0]))\n",
    "                num_dots = np.count_nonzero(masked_arr != 0.0)\n",
    "                num_expected = test_num_points[i]\n",
    "                \n",
    "                curr_diff = (num_dots - num_expected)**2\n",
    "                if (out_i % 50 == 0):\n",
    "                    print(f\"diff: {curr_diff}\")\n",
    "                # num_expected = np.count_nonzero(np.array(target.cpu().data[out_i][0]) == 0.0)\n",
    "\n",
    "                # print(np.array(target.cpu().data[out_i][0]).shape)\n",
    "                # if (i == t):\n",
    "                #     print(f\"num counted: {num_dots}, num expected:{num_expected}\")\n",
    "\n",
    "                #     im = Image.fromarray(np.array(data.cpu().data[out_i][0])*255)\n",
    "                #     im.show()\n",
    "\n",
    "                # print(f\"accuracy of this trial: {(num_dots - num_expected)/num_expected * 100}\")\n",
    "                avg_accuracy += curr_diff\n",
    "                i += 1\n",
    "    avg_accuracy = math.sqrt(avg_accuracy / size)\n",
    "    print(f\"RMSE accuracy: {avg_accuracy}\")\n",
    "\n",
    "# test(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(test_num_points[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "average loss: 0.0008085418376140296\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "average loss: 9.089544619200751e-05\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "average loss: 3.119631946901791e-05\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "average loss: 2.2650814571534283e-05\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "average loss: 1.9488190446281806e-05\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "average loss: 1.8025946701527573e-05\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "average loss: 1.6625406715320423e-05\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "average loss: 1.5859830455156043e-05\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "average loss: 1.4666788956674282e-05\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "average loss: 1.3962377124698833e-05\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "average loss: 1.3503035006579012e-05\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "average loss: 1.3190375284466427e-05\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "average loss: 1.2384598448988982e-05\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "average loss: 1.2056280866090674e-05\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "average loss: 1.1815163816208951e-05\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "average loss: 1.1158173947478645e-05\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "average loss: 1.1751082638511434e-05\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "average loss: 1.0616511644911952e-05\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "average loss: 1.0865250260394532e-05\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "average loss: 1.1318104952806607e-05\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "average loss: 9.811196832743008e-06\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "average loss: 9.695761946204584e-06\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "average loss: 9.82006258709589e-06\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "average loss: 1.00765628303634e-05\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "average loss: 9.452979611523915e-06\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "average loss: 1.0619976819725707e-05\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "average loss: 8.806185178400483e-06\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "average loss: 9.406610843143426e-06\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "average loss: 9.418166882824153e-06\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "average loss: 8.576219443057198e-06\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# test()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    # test(500)\n",
    "    # test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num counted: 0, num expected:8.0\n"
     ]
    }
   ],
   "source": [
    "i = 60\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_dataloader:\n",
    "        output = model(data)    \n",
    "        # INPUT\n",
    "        input_arr = np.array(data.cpu().data[i][0])*255\n",
    "        input = Image.fromarray(input_arr)\n",
    "        input.show()\n",
    "\n",
    "        # OUTPUT\n",
    "        output_arr = np.array(output.cpu().data[i][0])*255\n",
    "        output = Image.fromarray(output_arr)\n",
    "        output.show()\n",
    "\n",
    "        masked_arr = mask(output_arr/255, input_arr/255)\n",
    "        masked_arr = masked_arr\n",
    "        masked = Image.fromarray(masked_arr)\n",
    "        masked.show()\n",
    "\n",
    "        num_dots = np.count_nonzero(masked_arr != 0.0)\n",
    "        num_expected = test_num_points[i]\n",
    "        print(f\"num counted: {num_dots}, num expected:{num_expected}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:8807): Gtk-WARNING **: 13:31:33.708: Could not load a pixbuf from icon theme.\n",
      "This may indicate that pixbuf loaders or the mime database could not be found.\n"
     ]
    }
   ],
   "source": [
    "# test()\n",
    "# test()\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     # train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test()\n",
    "#     # test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "# print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c6b82f28f63ce523f5c26d92ac0b14ad6785c6a25a5c8f6e4c562a3fc2245a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
