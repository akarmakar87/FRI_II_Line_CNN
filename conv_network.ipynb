{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY WORKING IN THIS ONE\n",
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn import preprocessing as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = p.MinMaxScaler()\n",
    "\n",
    "INPUT_DATA_PATH = 'test_input.npy'\n",
    "OUTPUT_DATA_PATH = 'test_output.npy'\n",
    "x_input = np.load(INPUT_DATA_PATH)\n",
    "y_input = np.load(OUTPUT_DATA_PATH)\n",
    "\n",
    "x_min = x_input.min(axis=(1, 2), keepdims=True)\n",
    "x_max = x_input.max(axis=(1, 2), keepdims=True)\n",
    "x_input = (x_input - x_min)/(x_max-x_min)\n",
    "\n",
    "y_min = y_input.min(axis=(1, 2), keepdims=True)\n",
    "y_max = y_input.max(axis=(1, 2), keepdims=True)\n",
    "y_input = (y_input - y_min)/(y_max-y_min)\n",
    "\n",
    "# print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_input[x_input < 1] = 0\n",
    "\n",
    "\n",
    "# print(x_input[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(tensor):\n",
    "    for img in range(len(tensor)):\n",
    "            for channel in range(1):\n",
    "                    for h in range(100):\n",
    "                            for w in range(100):\n",
    "                                    if (tensor[img][channel][h][w] < 1):\n",
    "                                            tensor[img][channel][h][w] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "SPLIT_IDX = 15 #750\n",
    "x_train = torch.tensor(x_input[:SPLIT_IDX], dtype=torch.float).permute(0,3,1,2)\n",
    "x_test = torch.tensor(x_input[SPLIT_IDX:], dtype=torch.float).permute(0,3,1,2)\n",
    "\n",
    "y_train = torch.tensor(y_input[:SPLIT_IDX], dtype=torch.float).permute(0,3,1,2)\n",
    "y_test = torch.tensor(y_input[SPLIT_IDX:], dtype=torch.float).permute(0,3,1,2)\n",
    "\n",
    "threshold(x_train)                       \n",
    "threshold(x_test)\n",
    "threshold(y_train)                       \n",
    "threshold(y_test)\n",
    "\n",
    "class LinesDataset(TensorDataset):\n",
    "        def __init__(self, lines, labels):\n",
    "                self.lines = lines\n",
    "                self.labels = labels\n",
    "        def __len__(self):\n",
    "                return len(self.lines)\n",
    "        def __getitem__(self, idx):\n",
    "                line = torch.tensor(self.lines[idx])\n",
    "                label = torch.tensor(self.labels[idx])\n",
    "                sample = {\"Line\": line, \"Label\": label}\n",
    "                return sample\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(x_train,y_train) # create your datset\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=True) # create your dataloader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True) # create your dataloader\n",
    "\n",
    "print (len(train_dataloader))\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "#/home/bwilab/asha_ritu/line_model/conv_network.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CNN\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.cnn_stack = nn.Sequential( \n",
    "            # first layer: learning one person. \n",
    "            # kernel size = diameter of a dot (5 pixels) + padding (2 pixels) = (7,7)\n",
    "            # out_channels: 1 for now\n",
    "            # try changing the out_channels\n",
    "            # larger dataset and ReLU between each layer\n",
    "            # larger dataset, more layers, larger kernels\n",
    "\n",
    "            # find center pixel of each circle, count number of dots\n",
    "\n",
    "            # blob detect: looking for continuous regions of non-white pixels\n",
    "            # confidence regions. \n",
    "            # masking the image like in hw3\n",
    "            # \n",
    "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(6,6), stride=1, bias=True, padding=(2,2)), # input layer\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            # layer 2: learning two dots.\n",
    "            # kernel_size = diameter of two dots (10 pixels) + distance between them (2 pixels) + padding (2 pixels)\n",
    "            # there are 8 different orientations two people can be in, resulting in 8 output channels.\n",
    "            # two options: either increase kernel size and number of channels or\n",
    "            # pool -> lower image resolution and learn a smaller pattern\n",
    "\n",
    "            # nn.MaxPool2d()\n",
    "\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(14,14), stride=1, bias=True, padding=(7,7)),\n",
    "            # nn.ReLU(),\n",
    "            # final layer: out_channels=1, \n",
    "            # kernel size = ? ask Dr. Hart what a reasonable kernel size for the output layer could be.\n",
    "            nn.Conv2d(in_channels=8, out_channels=1, kernel_size=(13,13), padding=(6,6)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.cnn_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "batch_size = 4\n",
    "epochs = 5\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.CrossEntropyLoss() # can change this to another loss function\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "                  \n",
    "        # print(X)\n",
    "        pred = model(X)\n",
    "        # print(f\"pred:{pred}\")\n",
    "        # print(f\"target:{y}\")\n",
    "    \n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"y shape:{y.shape}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"loss: {loss:>7f}\")\n",
    "\n",
    "        # if batch % 50 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss, correct = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             # print(X.shape)\n",
    "#             pred = model(X)\n",
    "#             # print(pred.argmax(1).shape)\n",
    "#             # print(y.shape)\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "#             correct += (pred == y).type(torch.float).sum().item()\n",
    "#             # print(correct)\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     correct /= size\n",
    "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#   model.eval()\n",
    "#   test_loss = 0\n",
    "#   correct = 0\n",
    "#   avg_accuracy = 0\n",
    "#   with torch.no_grad():\n",
    "#     for data, target in test_dataloader:\n",
    "#       output = model(data)\n",
    "#       test_loss += loss_fn(output, target).item()\n",
    "\n",
    "#       img = 0\n",
    "#       h = 0\n",
    "#       w = 0        \n",
    "\n",
    "#       # compare the number of dots, not the pixel by pixel comparison\n",
    "\n",
    "#       for img in range (5):\n",
    "#         correct = 0\n",
    "#         # print (target[img][0].shape)\n",
    "#         for h in range (100):\n",
    "#           for w in range (100):\n",
    "#             # print(f\"output {output[img][0]}\")\n",
    "#             # print(f\"target {target[img][0]}\")\n",
    "#             if (target[img][0][h][w] == 0 and output[img][0][h][w] == 0):\n",
    "#               correct += 1\n",
    "\n",
    "        # print(f\"percentage of pixels correct for image {img}: {correct/10000}\")\n",
    "\n",
    "        # print (f\"num correct: {correct}\")\n",
    "\n",
    "      # num_correct = output.eq(target.data.view_as(output)).sum()\n",
    "      \n",
    "  # test_loss /= len(test_dataloader.dataset)\n",
    "  # test_losses.append(test_loss)\n",
    "  # print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "  #   test_loss, correct, len(test_dataloader.dataset),\n",
    "  #   100. * correct / len(test_dataloader.dataset)))\n",
    "\n",
    "  # avg_accuracy /= 1000\n",
    "  # print(f\"average accuracy: {avg_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.989012\n",
      "loss: 0.978897\n",
      "loss: 0.820703\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.180169\n",
      "loss: 2.571131\n",
      "loss: 0.992520\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.993140\n",
      "loss: 0.992720\n",
      "loss: 0.993680\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.992680\n",
      "loss: 0.993600\n",
      "loss: 0.993260\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.993640\n",
      "loss: 0.992760\n",
      "loss: 0.993140\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# test()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    # test()\n",
    "    # test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_dataloader:\n",
    "\n",
    "        i = 2\n",
    "        # print (target[0][0])\n",
    "        output = model(data)\n",
    "        # print(output)\n",
    "        # print(f\"{output[4][0]}\")\n",
    "        # with open('model_output.npy', 'wb') as f:\n",
    "        #     np.save(f, output)\n",
    "\n",
    "        # INPUT\n",
    "        input_arr = np.array(data[i][0])*255\n",
    "        input = Image.fromarray(input_arr)\n",
    "        input.show()\n",
    "        \n",
    "\n",
    "        # ACTUAL OUTPUT\n",
    "        output = [(1-x) for x in output]\n",
    "        arr = np.array(output[i][0])\n",
    "        arr[arr < 1] = 0\n",
    "        arr = arr * 255\n",
    "        im = Image.fromarray(arr)\n",
    "        im.show()\n",
    "\n",
    "        # EXPECTED OUTPUT\n",
    "        expected_arr = np.array(target[i][0]) * 255\n",
    "        # expected_arr[expected_arr < 1] = 0\n",
    "        # expected_arr = expected_arr * 255\n",
    "        # im_expected = Image.fromarray(expected_arr)\n",
    "        # im_expected.show()\n",
    "        \n",
    "\n",
    "       \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pyenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "952863ed7f17ffcbf80d30bf455f69fb2e26b6b85815b8fac0ebd69316f2e222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
